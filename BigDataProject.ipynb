{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For this project, we will create an anime recommendation system. A user will input his/hers genre of choice and the model will output the top 10 anime of that genre, along with their descriptions. "
      ],
      "metadata": {
        "id": "vIYecBPig7id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Data used for this project consist of about 17.562 anime and the preference from 325.772 different users. \n",
        "\n",
        "Datasets Description:\n",
        "\n",
        "The Anime_df has 16214 rows and 5 columns:\n",
        "\n",
        "* MAL_IDName: the anime id (Primary key) \n",
        "* Name: the title of the anime\n",
        "* Score: the average rating (however we will drop that column)\n",
        "* Genres: the genres of each anime entry\n",
        "* Synopsis: a brief description of the anime.\n",
        "\n",
        "The Ratings_df has 57633278 rows and 3 columns:\n",
        "\n",
        "* user_id: Id of the user who left a rating\n",
        "* anime_id: anime id (foreigh key)\n",
        "* rating: rating from 1 to 10"
      ],
      "metadata": {
        "id": "y87tugy0hY-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Big Data problem surrounding this project is Volume, because of the large number of rows of the Ratings_df. Moreover, we will perform analysis on batch data."
      ],
      "metadata": {
        "id": "g7uobEicjeOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting Up Pyspark"
      ],
      "metadata": {
        "id": "s4KL-UuJ9Z1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "zEjQSkYX9XFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "id": "N6sVStic9diM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "jaHtXAI19d-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUmcNxSRGrJN"
      },
      "outputs": [],
      "source": [
        "from pyspark.context import SparkContext, SparkConf\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Window\n",
        "from pyspark.sql import Row\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import explode \n",
        "from pyspark.sql.functions import rank\n",
        "from pyspark.sql.functions import col \n",
        "from pyspark.sql.functions import avg \n",
        "from pyspark.sql.functions import split\n",
        "from pyspark.sql.functions import round\n",
        "import time\n",
        "from functools import reduce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu6udssNDvc0"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master('local[5]')\\\n",
        "        .appName('Anime recommendation system')\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a8MwZw0NXYz"
      },
      "source": [
        "#Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAnuZ6kSfmz1"
      },
      "outputs": [],
      "source": [
        "Anime_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/anime_with_synopsis.csv', header=True, inferSchema=True)\n",
        "Ratings_df = spark.read.csv('/content/drive/MyDrive/Colab Notebooks/rating_complete.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiz_0rOzU_TI"
      },
      "outputs": [],
      "source": [
        "def withColumnRenamed(existingName: any, newName: any): \n",
        "  DataFrame\n",
        "\n",
        "Anime_df = Anime_df.withColumnRenamed(\"sypnopsis\",\"synopsis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnT-STcCmvwq"
      },
      "outputs": [],
      "source": [
        "Anime_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fq8OZfa_MU9x"
      },
      "outputs": [],
      "source": [
        "Ratings_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL9rycvMmkXf"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD0aomsplurj"
      },
      "source": [
        "For the project, we will use the Anime_df as well as the Ratings_df. We will drop the 'Score' column for the Anime_df as we will create a model that will aggregate the ratings of the Ratings_df and then output the score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyFl05oUmL8e"
      },
      "outputs": [],
      "source": [
        "Anime_df = Anime_df.drop('Score')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOeAhsJEn8yn"
      },
      "source": [
        "Then we will explode the genre column, so each row have just one gerne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75sawhFrosHy"
      },
      "outputs": [],
      "source": [
        "Anime_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofETH7OAUUpa"
      },
      "source": [
        "Checking for Null values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwx5XoMzUDJT"
      },
      "outputs": [],
      "source": [
        "Anime_df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in Anime_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wdklzyIUSiy"
      },
      "outputs": [],
      "source": [
        "Ratings_df.select([F.count(F.when(F.isnull(c), c)).alias(c) for c in Ratings_df.columns]).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qd6918iUu8Q"
      },
      "source": [
        "As we can see, Anime_df has 8 null values in the synopsis column, however, we will not drop these rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCWgxIJjPaKJ"
      },
      "source": [
        "Printing the shapes of the above dataframes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHW0FkX0Ovas"
      },
      "outputs": [],
      "source": [
        "print(f'Anime_df has {Anime_df.count()} rows and {len(Anime_df.columns)} columns')\n",
        "print(f'Ratings_df has {Ratings_df.count()} rows and {len(Ratings_df.columns)} columns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU6SxoAPXq7G"
      },
      "source": [
        "Because the genre column of the Anime_df consists of multiple genres per anime, we will split them, resulting in each row containing only one genre per anime."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting each entry of the 'Genres' column to a list of strings, for them to be exploded\n",
        "Anime_df = Anime_df.withColumn('Genres', split(Anime_df['Genres'], ', '))\n",
        "Anime_df.show()"
      ],
      "metadata": {
        "id": "Hz_JEb05hbQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explode the 'Genre' column"
      ],
      "metadata": {
        "id": "ClkP0PikvFlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_df.printSchema()"
      ],
      "metadata": {
        "id": "AS8qtexuz8XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_df = Anime_df.select( '*' , explode('Genres').alias('Genres2'))"
      ],
      "metadata": {
        "id": "AOTNLjqnwJQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_df = Anime_df.drop('Genres')\n",
        "Anime_df = Anime_df.withColumnRenamed('Genres2', 'Genres')"
      ],
      "metadata": {
        "id": "v25D9iR806Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_df.show()"
      ],
      "metadata": {
        "id": "c9Af8F8DvDoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_IN9Olu4Krf"
      },
      "outputs": [],
      "source": [
        "Ratings = Ratings_df.select(['anime_id' , 'rating'])\n",
        "Ratings.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of the Genre column"
      ],
      "metadata": {
        "id": "jEuPijYdcsyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group column by distinct values and count occurrences\n",
        "df_count = Anime_df.groupBy('Genres').count()\n",
        "\n",
        "# Convert DataFrame to Pandas\n",
        "df_pd = df_count.toPandas()\n",
        "\n",
        "# Create bar plot\n",
        "df_pd.plot(kind='bar', x='Genres', y='count' , figsize=(10,6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Puc1lNtPcohH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can observe, the Genre column is unbalanced as the counts differ. Comedy appears to be the most dominant genre of the dataframe contrastly with '7.14' and '6.45' to be the least popular genres."
      ],
      "metadata": {
        "id": "p-9V5JCJeFu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Model Design "
      ],
      "metadata": {
        "id": "9uuz7w6ixaOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first phase, we will process the 'Ratings' Dataframe, to compute the average rating of each anime. To achieve this, we will first turn the dataframe into an RDD. Then, with a Map function, we will create key-value pairs, where keys will be the anime ids and values their corresponding ratings. Afterward, the key-value pairs will be grouped by key creating key-lists of value pairs that then be input into a reduce function. Lastly, the reduce function will take the key-lists of values and output the average rating of each key."
      ],
      "metadata": {
        "id": "o_Wc10IlYl1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_ratings(df):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Turning the dataframe into an RDD\n",
        "    RDD = df.rdd\n",
        "\n",
        "# Map function:\n",
        "    # Performing the map function in parallel on each chunk, in which it will create key-value pairs, key: anime_id, value: rating:\n",
        "    Map = RDD.map(lambda x : (x[0], x[1]) )\n",
        "\n",
        "#Group-By key function:\n",
        "    # Using the groupByKey() function to group the key-value pairs by key, creating key-lists of values pairs:\n",
        "    GroupBy = Map.groupByKey()\n",
        "\n",
        "#Reduce function:\n",
        "    # Computing the average rating for each key:\n",
        "    Reduce = GroupBy.mapValues(lambda x : sum(x) / len(x)) \n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Map-Reduce time: ', end_time - start_time)\n",
        "\n",
        "    # Converting the RDD back to a DataFrame\n",
        "    Avrg_df = Reduce.toDF(['anime_id', 'score'])\n",
        "\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Processing time: ', end_time - start_time)\n",
        "\n",
        "    return Avrg_df"
      ],
      "metadata": {
        "id": "y0oiXQJcBVcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Average_Ratings = average_ratings(Ratings)\n",
        "Average_Ratings.show()"
      ],
      "metadata": {
        "id": "6uUKiSvbU4HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Approach to the First Model:"
      ],
      "metadata": {
        "id": "BKlrO3FtRxtL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another approach to compute the average rating of each anime in the Ratings Dataframe, would be to first group by the anime_id column. This is a similar procedure to the Map and GroupByKey functions. The output is key-lists of values pairs, where the keys are the anime ids paired with lists of their corresponding ratings. Then, we will apply a reduction function by computing the average rating of each anime"
      ],
      "metadata": {
        "id": "nI76XGxZR3Fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def average_ratings(df):\n",
        "\n",
        "    # We will also compute the processing time\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Grouping by the anime_id\n",
        "    df_grouped = df.groupBy('anime_id')\n",
        "    # we will aggregate the results computing the average rating of each transaction\n",
        "    df_avg = df_grouped.agg(avg('rating').alias('Score'))\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Processing time: ', end_time - start_time)\n",
        "\n",
        "\n",
        "    return df_avg"
      ],
      "metadata": {
        "id": "DVo1GV43SOBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Average_Ratings = average_ratings(Ratings)\n",
        "Average_Ratings.show()"
      ],
      "metadata": {
        "id": "7A09WHiMSfdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step is to join the two tables, Average_Ratings and Anime_df, on anime_id."
      ],
      "metadata": {
        "id": "qffUYxXdcIbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_DF = Anime_df.join(Average_Ratings, Anime_df.MAL_ID == Average_Ratings.anime_id, 'inner')\n",
        "Anime_DF = Anime_DF.drop('MAL_ID')\n",
        "Anime_DF.show()"
      ],
      "metadata": {
        "id": "hmlJrLPUcfex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second Model Design "
      ],
      "metadata": {
        "id": "ll08iKI2k-__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the second phase, we will collect the top 10 animes of each genre based on their score and store them in a Dataframe. This will be achieved by first turning the Anime_DF into an RDD and then applying a map function emitting key-value pairs, the key will be the genre and the values will be the name, synopsis, and score. Then, we will group by genre, creating key-lists of values pairs. Lastly, we will sort the key-lists of values pairs by score and save the top 10 animes in a new dataframe."
      ],
      "metadata": {
        "id": "77_vfuBWfxkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_10_by_genre(df):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Converting DataFrame to RDD\n",
        "    rdd = df.rdd\n",
        "\n",
        "# Map function:\n",
        "    # Performing the map function, in which it will creat key-value pairs, key: genre , value: ( Name, Synopsis , Score ):\n",
        "    Map = rdd.map(lambda x: (x[2], (x[0], x[1], x[4])))\n",
        "\n",
        "# Group by key:\n",
        "    # Grouping by key, and creating key-lists of value pairs\n",
        "    List_of_values = Map.groupByKey().mapValues(lambda x: list(x))\n",
        "\n",
        "# Sorting phase:\n",
        "\n",
        "    # We will create an empty list to append the top 10 animes of each genre and then use it to create a dataframe:\n",
        "    top_10_list = []\n",
        "    \n",
        "    # Creating a for loop, firstly sorting the values of each genre by their score, collecting the top 10 and lastly appending the results in the list\n",
        "    for genre, values in List_of_values.collect():\n",
        "        top10 = sorted( values, key = lambda x: x[2] , reverse=True )[:10]\n",
        "\n",
        "        for row in top10:\n",
        "            top_10_list.append(Row(genre, row[0] , row[1] , row[2]))\n",
        "\n",
        "    Top10_DF = spark.createDataFrame(top_10_list)\n",
        "\n",
        "    # Processing the Dataframe\n",
        "    Top10_DF = Top10_DF.withColumnRenamed('_1', 'Genre')\n",
        "    Top10_DF = Top10_DF.withColumnRenamed('_2', 'Name')\n",
        "    Top10_DF = Top10_DF.withColumnRenamed('_3', 'Synopsis')\n",
        "    Top10_DF = Top10_DF.withColumn('Score', round(Top10_DF['_4'], 2))\n",
        "    Top10_DF= Top10_DF.drop('_4')\n",
        "\n",
        "    end_time = time.time()\n",
        "    print('Processing time: ', end_time - start_time)\n",
        " \n",
        "    return Top10_DF\n"
      ],
      "metadata": {
        "id": "TZruehm3p_XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Top10 = top_10_by_genre(Anime_DF)\n",
        "Top10.show()"
      ],
      "metadata": {
        "id": "VpY_lDpHbOY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will create a function, that will asks a user for genres and then return the top animes that belong to those genres."
      ],
      "metadata": {
        "id": "r5xXP_iJ9FSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Anime_Recommendation_System(df):\n",
        "\n",
        "  print('-----------------------------------------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "  print('Welcome to the Anime Recommendation System!\\n')\n",
        "  print('You will be asked to input your genre of choice and the system will return the top 10 animes that correspond to that genre')\n",
        "  print('Lets get started!\\n')\n",
        "  print('-----------------------------------------------------------------------------------------------------------------------------------------------------------\\n')\n",
        "\n",
        "  Genres = input( 'What genre you are interested in? \\n')\n",
        "  \n",
        "  df.filter(df.Genre == Genres).select('Name', 'Synopsis').show()"
      ],
      "metadata": {
        "id": "43TPw62t9FzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Anime_Recommendation_System(Top10)"
      ],
      "metadata": {
        "id": "1WVRGqJp9Qnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2l7aM7CLR5i"
      },
      "source": [
        "#Bibliography"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcWN5SqILVkt"
      },
      "source": [
        "Setting up Pyspark on google collab: \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2020/11/a-must-read-guide-on-how-to-work-with-pyspark-on-google-colab-for-data-scientists/\n",
        "\n",
        "https://stackoverflow.com/questions/55240940/error-while-installing-spark-on-google-colab\n",
        "\n",
        "Datasets:\n",
        "\n",
        "https://www.kaggle.com/datasets/hernan4444/anime-recommendation-database-2020\n",
        "\n",
        "Google Cloud Dataproc:\n",
        "\n",
        "https://holowczak.com/getting-started-with-pyspark-on-google-cloud-platform-dataproc/9/\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}